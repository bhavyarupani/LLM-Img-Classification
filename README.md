# Vision-and-Language-Models-for-Image-Classification

### Thesis-Computer-VIsion

This project integrates cutting-edge vision and language models to enhance image classification performance on benchmark datasets: **CIFAR-10**, **CIFAR-100**, and **ImageNet-1K**. WHile also create text enriched datasets for each of the Image datasets mentioned. To improve dataset quality and classification accuracy. The approach combines **state-of-the-art (SOTA) image classification models** (BEiT, Swin Transformer, EVA-02) with **BLIP-generated captions** and fine-tunes **BART for classification**.

## ðŸš€ Project Overview

This project explores a novel workflow combining Vision Models and Vision-Language Models. The approach involves:
1. **Vision Models**: BEiT, Swin Transformer, EVA-02
2. **Captioning Model**: BLIP (Bootstrapped Language-Image Pre-training)
3. **Classification Model**: Fine-tuned BART at the end of each pipeline.

### Key Highlights
- **CIFAR-10**: Using BEiT, BLIP, and fine-tuned BART.
- **CIFAR-100**: Using Swin Transformer, BLIP, and fine-tuned BART.
- **ImageNet-1K**: Using EVA-02, BLIP, and fine-tuned BART.
- **Dataset Enrichment:** Captions generated using BLIP are combined with classifier-generated structured text.
- **Semantic Similarity Analysis:** Ensures meaningful alignment between image descriptions and ground-truth labels.
- **Fine-Tuning BART:** Uses enriched textual data for **text-based classification**.
- **Evaluation on SOTA Benchmarks:** Performance compared against traditional vision-only approaches.

## ðŸ“‚ Workflow for Dataset Creation

The datasets were enriched by combining:
1. **Classification Descriptions**: Generated by BEiT, Swin Transformer, and EVA-02.
2. **Captioning Descriptions**: Generated by BLIP, providing semantic richness.

### Workflow Summary
1. **Load Datasets**: CIFAR-10, CIFAR-100, ImageNet-1K.
2. **Generate Predictions**: Vision models predict classification labels for each image.
3. **Generate Captions**: BLIP generates contextual captions for enhanced understanding.
4. **Combine Descriptions**: Merge classification labels and captions into a multimodal input.
5. **Create JSON Dataset**: Store enriched outputs and true labels for downstream tasks.
6. **Fine-Tune BART**: Train the BART model on the enriched datasets for final classification.